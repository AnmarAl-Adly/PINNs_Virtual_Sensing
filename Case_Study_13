import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from torch.optim import LBFGS
import time
import csv
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader

# Check if GPU is available and set the default device accordingly
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f'Using device: {device}')
# Clears cached memory
torch.cuda.empty_cache() 
# Start timing
start_time = time.time()
import torch.nn as nn

#Loss_Functions
def compute_initial_condition_loss_l(model, x, t, p):
    initial_points = torch.cat([x, t, p], dim=1)
    # Compute model prediction at initial points
    u_initial = model(initial_points)
    # Compute the loss for initial condition u(x, 0) = 0
    loss_initial_u = (u_initial ** 2).mean()
    initial_condition_loss_l = loss_initial_u
    return initial_condition_loss_l

def compute_initial_condition_loss_lt(model, x, t, p):
    # Ensure t has gradient tracking enabled
    t.requires_grad_(True)
    # Concatenate inputs along feature dimension
    initial_points_grad = torch.cat([x, t, p], dim=1)
    # Compute model prediction at initial points
    u_initial_grad_l = model(initial_points_grad)
    # Compute the gradient of u with respect to t
    u_t_initial_l = torch.autograd.grad(u_initial_grad_l, t, grad_outputs=torch.ones_like(u_initial_grad_l), create_graph=True)[0]
    # Compute the loss for initial condition derivative u_t(x, 0) = 0
    loss_initial_u_t = (u_t_initial_l ** 2).mean()
    # Total initial condition loss
    initial_condition_loss_lt = loss_initial_u_t
    return initial_condition_loss_lt

def compute_residual_loss(model, x, t, p, L, v, mu, EI):
    x.requires_grad_(True)
    t.requires_grad_(True)
    p.requires_grad_(True)
    residual_points = torch.cat([x, t, p], dim=1)
    u = model(residual_points)
    # Compute first and second, third and forth derivatives w.r.t x and t
    u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]
    u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]
    u_xxx = torch.autograd.grad(u_xx, x, grad_outputs=torch.ones_like(u_xx), create_graph=True)[0]
    u_xxxx = torch.autograd.grad(u_xxx, x, grad_outputs=torch.ones_like(u_xxx), create_graph=True)[0]
    u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]
    u_tt = torch.autograd.grad(u_t, t, grad_outputs=torch.ones_like(u_t), create_graph=True)[0]
    # Compute moving load effect
    load_position = (x / L) - ((v / L) * t)
    delta = 1.0 / (0.05 * torch.sqrt(torch.tensor(np.pi))) * torch.exp(-load_position.pow(2) / (0.05**2))
    # Loss for PDE
    loss_res = ((mu * u_tt + EI * u_xxxx - p.squeeze() * delta) ** 2).mean()
    return loss_res

def compute_boundary_loss_u_r(model, x_0, t_b, p_b):
    x_0.requires_grad_(True)
    t_b.requires_grad_(True)
    u_r = model(torch.cat([x_0, t_b, p_b], dim=1))
    # Boundary loss for deflection and curvature being zero at the boundaries
    loss_boundary_u_r = u_r.pow(2).mean()
    return loss_boundary_u_r

def compute_boundary_loss_u_l(model, x_L, t_b, p_b):
    x_L.requires_grad_(True)
    t_b.requires_grad_(True)
    u_l = model(torch.cat([x_L, t_b, p_b], dim=1))
    # Boundary loss for deflection and curvature being zero at the boundaries
    loss_boundary_u_l = u_l.pow(2).mean()
    return loss_boundary_u_l

def compute_boundary_loss_uu_r(model, x_0, t_b, p_b):
    x_0.requires_grad_(True)
    t_b.requires_grad_(True)
    u_0 = model(torch.cat([x_0, t_b, p_b], dim=1))
    # Compute second spatial derivatives at the boundaries
    u_xx_0 = torch.autograd.grad(torch.autograd.grad(u_0, x_0, grad_outputs=torch.ones_like(u_0), create_graph=True)[0], x_0, grad_outputs=torch.ones_like(u_0), create_graph=True)[0]
    # Boundary loss for deflection and curvature being zero at the boundaries
    loss_boundary_uu_r = u_xx_0.pow(2).mean()
    return loss_boundary_uu_r

def compute_boundary_loss_uu_l(model, x_L, t_b, p_b):
    x_L.requires_grad_(True)
    t_b.requires_grad_(True)
    u_L = model(torch.cat([x_L, t_b, p_b], dim=1))
    # Compute second spatial derivatives at the boundaries
    u_xx_L = torch.autograd.grad(torch.autograd.grad(u_L, x_L, grad_outputs=torch.ones_like(u_L), create_graph=True)[0], x_L, grad_outputs=torch.ones_like(u_L), create_graph=True)[0]
    # Boundary loss for deflection and curvature being zero at the boundaries
    loss_boundary_uu_l = u_xx_L.pow(2).mean()
    return loss_boundary_uu_l

class PINN(torch.nn.Module):
    def __init__(self):
        super(PINN, self).__init__()
        self.net = torch.nn.Sequential(
            torch.nn.Linear(3, 64),
            torch.nn.Tanh(),
            torch.nn.Linear(64, 64),
            torch.nn.Tanh(),
            torch.nn.Linear(64, 64),
            torch.nn.Tanh(),
            torch.nn.Linear(64, 64),
            torch.nn.Tanh(),
            torch.nn.Linear(64, 64),
            torch.nn.Tanh(),
            torch.nn.Linear(64, 1)
        )

    def forward(self, x):
        return self.net(x)

# Initialize model, optimizer, and define parameters
model = PINN().to(device)
# Define optimizers
adam_optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
lbfgs_optimizer = torch.optim.LBFGS(model.parameters(), lr=1e-5, max_iter=100, tolerance_grad=1e-5, tolerance_change=1e-9, history_size=50)

# Set iteration thresholds
N = 30000  # Switch to LBFGS after N iterations
N_max = 60000  # Maximum iterations
convergence_threshold = 1e-16  # Define a convergence criterion
L, v, mu, EI = 1.00, 0.4115, (289*17.28*1e-5), (1033610000*1e-5)/17.28**3
p = 0.650
T = L / v

max_x = L  # Beam length
max_t = T  # Assuming maximum time of interest

# Prepare data points for residual and boundary conditions
torch.manual_seed(2000)
np.random.seed(2000)
x_res = torch.rand(5000, 1).to(device) * L
t_res = torch.rand(5000, 1).to(device) * T
p_res = torch.rand(5000, 1).to(device)*p  

# Combine the tensors into a single dataset
dataset = TensorDataset(x_res, t_res, p_res)
# Define the batch size
batch_size = 5000
# Create a DataLoader to handle batching
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Prepare data points for boundary conditions
t_boundary = torch.rand(3000, 1).to(device) * T
x_boundary_0 = torch.zeros_like(t_boundary).to(device)
x_boundary_L = L * torch.ones_like(t_boundary).to(device)
p_boundary =  torch.rand(3000, 1).to(device)*p 

# Prepare data points for initial boundary conditions
x_initial = torch.rand(3000, 1).to(device) * L
t_initial_l = torch.zeros_like(x_initial).to(device)
t_initial_r = T * torch.ones_like(x_initial).to(device)
p_initial = torch.rand(1000, 1).to(device)*p 

# Loss history for plotting
loss_residual_history = []
loss_boundary_u_r_history = []
loss_boundary_u_l_history = []
loss_boundary_uu_r_history = []
loss_boundary_uu_l_history = []
loss_initial_history_l = []
loss_initial_history_lt = []
total_loss_history = []
iteration_history = []


# Define the CSV file name
csv_file_path = 'xtp.csv'
# Open the CSV file in write mode
with open(csv_file_path, mode='w', newline='') as file:
    # Create a CSV writer
    writer = csv.writer(file)

    # Write the header (column names)
    writer.writerow(['Iteration', 'Total Loss', 'Loss Residual', 'Loss Boundary_u_r', 'Loss Boundary_u_l', 'Loss Boundary_uu_r', 'Loss Boundary_uu_l', 'Loss Initial_0', 'Loss Initial_1'])

    # Training loop
    prev_loss = float('inf')
    for j in range(1, N_max + 1):
        if j <= N:
            # Adam optimizer phase
            batch_losses = []
            for x_batch, t_batch in dataloader:
                # Compute losses with gradients for optimization
                loss_residual = compute_residual_loss(model, x_batch, t_batch, p, L, v, mu, EI).to(device)
                loss_boundary_u_r = compute_boundary_loss_u_r(model, x_boundary_0, t_boundary).to(device)
                loss_boundary_u_l = compute_boundary_loss_u_l(model, x_boundary_L, t_boundary).to(device)
                loss_boundary_uu_r = compute_boundary_loss_uu_r(model, x_boundary_0, t_boundary).to(device)
                loss_boundary_uu_l = compute_boundary_loss_uu_l(model, x_boundary_L, t_boundary).to(device)
                loss_initial = compute_initial_condition_loss(model, x_initial, t_initial).to(device)
                loss_initial_t = compute_initial_condition_loss_lt(model, x_initial, t_initial).to(device)

                total_loss = loss_residual + loss_boundary_u_r + loss_boundary_u_l + loss_boundary_uu_r + loss_boundary_uu_l + loss_initial + loss_initial_t
                adam_optimizer.zero_grad()
                total_loss.backward()
                adam_optimizer.step()
                batch_losses.append(total_loss.item())

        else:
            # LBFGS optimizer phase
            def closure():
                lbfgs_optimizer.zero_grad()
                loss_residual = compute_residual_loss(model, x_res, t_res, p, L, v, mu, EI).to(device)
                loss_boundary_u_r = compute_boundary_loss_u_r(model, x_boundary_0, t_boundary).to(device)
                loss_boundary_u_l = compute_boundary_loss_u_l(model, x_boundary_L, t_boundary).to(device)
                loss_boundary_uu_r = compute_boundary_loss_uu_r(model, x_boundary_0, t_boundary).to(device)
                loss_boundary_uu_l = compute_boundary_loss_uu_l(model, x_boundary_L, t_boundary).to(device)
                loss_initial = compute_initial_condition_loss(model, x_initial, t_initial).to(device)
                loss_initial_t = compute_initial_condition_loss_lt(model, x_initial, t_initial).to(device)

                total_loss = loss_residual + loss_boundary_u_r + loss_boundary_u_l + loss_boundary_uu_r + loss_boundary_uu_l + loss_initial + loss_initial_t
                total_loss.backward()
                return total_loss

            lbfgs_optimizer.step(closure)

        # Store loss values and iteration number
        loss_residual_history.append(loss_residual.item())
        loss_boundary_u_r_history.append(loss_boundary_u_r.item())
        loss_boundary_u_l_history.append(loss_boundary_u_l.item())
        loss_boundary_uu_r_history.append(loss_boundary_uu_r.item())
        loss_boundary_uu_l_history.append(loss_boundary_uu_l.item())
        loss_initial_history_l.append(loss_initial_l.item())
        loss_initial_history_lt.append(loss_initial_lt.item())
        total_loss_history.append(total_loss.item())
        iteration_history.append(j)

        # Convergence check
        loss_change = prev_loss - total_loss.item()
        if abs(loss_change) < convergence_threshold:
            print(f"Converged after {j} iterations.")
            break
        prev_loss = total_loss.item()
        writer.writerow([j, total_loss.item(), loss_residual.item(), loss_boundary_u_r.item(), loss_boundary_u_l.item(), loss_boundary_uu_r.item(), loss_boundary_uu_l.item(), loss_initial_l.item(), loss_initial_lt.item()])
        # Optional: Print the loss every few iterations
        if j % 100 == 0:
            print(f"Iteration {j}, Total Loss: {total_loss.item()}, loss_residual: {loss_residual.item()}, loss_boundary_u_r: {loss_boundary_u_r.item()}, loss_boundary_u_l: {loss_boundary_u_l.item()}, loss_boundary_uu_r: {loss_boundary_uu_r.item()}, loss_boundary_uu_l: {loss_boundary_uu_l.item()}, loss_initial_l: {loss_initial_l.item()}, loss_initial_lt: {loss_initial_lt.item()}, loss_initial_r: {loss_initial_r.item()}, loss_initial_rt: {loss_initial_rt.item()}")

# End timing
end_time = time.time()
PATH = "xtp.pth"
torch.save({
    'model_state_dict': model.state_dict(),
    'adam_optimizer_state_dict': adam_optimizer.state_dict(),
    'lbfgs_optimizer_state_dict': lbfgs_optimizer.state_dict(),
}, PATH)
# Calculate and print the elapsed time
elapsed_time = end_time - start_time
print(f"Training completed in {elapsed_time:.2f} seconds.")
model.eval()
